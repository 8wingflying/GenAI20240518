# LLM(Large Language Models)
- [LLMæ˜¯ä»€éº¼ï¼Ÿè·ŸAIçš„é—œè¯ç‚ºä½•ï¼Ÿå¤§åž‹èªžè¨€æ¨¡åž‹è¦é¢å°ä»€éº¼æŒ‘æˆ°ï¼Ÿä¸€æ–‡çœ‹æ‡‚](https://www.bnext.com.tw/article/76864/what-is-the-meaning-of-llm)

## æ•…äº‹çˆ†ç™¼åœ¨2017 
![Transformer(2017)](Transformer(2017).png)


## LLMç™¼å±•å² Evolutionary Tree

![LLMç™¼å±•å²](LLM_Tree.png)
- o	Open-Source
- o	Closed-Source
- o	Encoder-Only
- o	Encoder-Decoder
- o	Decoder- Only
### 2024
- Chat GLM(6B) [æ™ºè­œæ¸…è¨€-ChatGLM](https://chatglm.cn/)
  - [å®˜æ–¹èªªæ˜Ž](https://github.com/THUDM/ChatGLM-6B)
  - ChatGLM-6B æ˜¯ä¸€å€‹é–‹æºçš„ã€æ”¯æŒä¸­è‹±é›™èªžçš„å°è©±èªžè¨€æ¨¡åž‹ï¼ŒåŸºæ–¼ General Language Model (GLM) æž¶æ§‹ï¼Œå…·æœ‰ 62 å„„åƒæ•¸ã€‚
  - ChatGLM-6Bçµåˆæ¨¡åž‹é‡åŒ–æŠ€è¡“ï¼Œä½¿ç”¨è€…å¯ä»¥åœ¨æ¶ˆè²»ç´šçš„é¡¯å¡ä¸Šé€²è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–ç´šåˆ¥ä¸‹æœ€ä½Žåªéœ€ 6GB é¡¯å­˜ï¼‰ã€‚
  - ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€è¡“ï¼Œé‡å°ä¸­æ–‡å•ç­”å’Œå°è©±é€²è¡Œäº†å„ªåŒ–ã€‚
  - ç¶“éŽç´„ 1T è­˜åˆ¥å­—çš„ä¸­è‹±é›™èªžè¨“ç·´ï¼Œè¼”ä»¥ç›£ç£å¾®èª¿ã€å›žé¥‹è‡ªåŠ©ã€äººé¡žå›žé¥‹å¼·åŒ–å­¸ç¿’ç­‰æŠ€è¡“çš„åŠ æŒï¼Œ62 å„„åƒæ•¸çš„ ChatGLM-6B å·²ç¶“èƒ½ç”Ÿæˆç›¸ç•¶ç¬¦åˆäººé¡žåå¥½çš„å›žç­”ã€‚
  - chatglm.cn é«”é©—æ›´å¤§è¦æ¨¡çš„ ChatGLM æ¨¡åž‹ã€‚ 
- Google Flan-UL2(20B)
- Meta LLaMA(65B)
  - LLaMA(Large Language Model Meta AI)æ˜¯Meta AIå…¬å¸æ–¼2023å¹´2æœˆæŽ¨å‡ºçš„å¤§åž‹èªžè¨€æ¨¡åž‹
  - å› å…¶é–‹æºçš„ç‰¹æ€§èˆ‡å…¶æ“æœ‰ChatGPTç›¸ä¼¼çš„æ•ˆèƒ½ï¼Œä½¿å¾—è©²æ¨¡åž‹å—åˆ°è¨±å¤šäººå–œæ„›
  - è¨±å¤šäººæœƒåŠ ä»¥å¾®èª¿ä»¥é”æˆé å®šçš„ç›®çš„ï¼Œç”šè‡³æœ‰è¨±å¤šäººå°‡é€™ç¨®æ¨¡åž‹è¦–ç‚ºã€Œæœ¬åœ°ç«¯ChatGPTã€æˆ–ã€Œé–‹æºChatGPTã€
  - [ChatGPTçš„æŒ‘æˆ°è€…LLaMA(ä¸Š) - ç›®å‰æœ€å¼·å¤§çš„é–‹æºèªžè¨€æ¨¡åž‹LLaMAç©¶ç«Ÿåšäº†ä»€éº¼](https://ithelp.ithome.com.tw/articles/10338745)
  - [ChatGPTçš„æŒ‘æˆ°è€…LLaMA(ä¸‹) - ç”¨RLHFèˆ‡QLoRAèª¿æ•´å¤§åž‹èªžè¨€æ¨¡åž‹](https://ithelp.ithome.com.tw/articles/10339382)
- Google Bard|â€ŽGemini  [â€ŽGemini:ç²å–å‰µæ„éˆæ„Ÿï¼Œæå‡å·¥ä½œæ•ˆçŽ‡](https://gemini.google.com/?hl=zh-cn)
  - Bard AI æ˜¯ç”± Google åŸºæ–¼ LaMDA(Language Model for Dialogue Applications)æ‰€é–‹ç™¼çš„ AI èŠå¤©æ©Ÿå™¨äºº
  - å®ƒå¯ä»¥åƒæœå°‹å¼•æ“Žä¸€æ¨£å›žç­”ç°¡å–®çš„å•é¡Œï¼Œä¸¦å°è¤‡é›œçš„å•é¡Œæä¾›å…¨é¢ä¸”å„ªè³ªçš„ç­”æ¡ˆã€‚
  - 2023å¹´5æœˆï¼ŒGoogleå®£å¸ƒæ›´æ”¹Bardåº•å±¤æ¨¡åž‹ï¼Œå¾žLaMDAèª¿æ•´ç‚ºPaLM 2ï¼Œèƒ½åŠ›è®Šå¾—æ›´å¼·
  - Google Bardæ›´åç‚ºâ€ŽGemini
- ðŸ‘OpenAI GPT-4 
- Jurassic-2 
- Claude 3
  - ç”±OpenAIå‰æˆå“¡å»ºç«‹çš„ Anthropicå…¬å¸é–‹ç™¼
  - ç‰¹è‰²:(æ¯”ChatGPTåŽ²å®³å‘¢ï¼Ÿ)
    - å…è²»ä½¿ç”¨AIå°è©±æ©Ÿå™¨äºº
    - è³‡æ–™æ›´æ–°åˆ°2023å¹´åˆ
    - æ”¯æ´è¼¸å…¥100K token(è©žèªžæˆ–å­—ç¬¦çš„åŸºæœ¬å–®ä½)çš„è³‡æ–™å…§å®¹(GPT-4åªæœ‰32K token)
    - å¯ä»¥ä¸Šå‚³PDFæª”æ¡ˆå–å‡ºæ–‡å­—å’Œç”¢ç”Ÿæ‘˜è¦

### 2023
- ðŸ‘ChatGPT(20B)
-	ST-MoE
-	UL2
-	Flan T5
-	TK
- InstructGPT
- Sparrow
- OPT-IML
- BLOOMZ / BLOOM(176B)
- OPT
- Chinchilla
- Galactica 
- YaLM / PaLM(540B)
- Minerva
- LaMDA(173B)
- Flan PaLM
- Anthropic LM-v4-s3
- GPT-NeoX

### 2022
- GLM(General Language Model)
  - è«–æ–‡[GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)
  - [å®˜æ–¹GITHUBç¶²å€](https://github.com/THUDM/GLM)
- Switch 
- CodeX
- MT-NLG(530B)
- GLaM
- Jurassic-1
- Gopher
- ERNIE3.0
- LM
- Cohere
- GPT-J
- GPT-Neo

### 2021
- [Google ELECTRA](https://github.com/google-research/electra) 
- DeBERTa 
- mT5
- T0 
- GPT-3(175B)

### 2020
- RoBERTa
- ALBERT
- Distill BERT
- ERNIE 
- BART
- T5
- XL Net
- GPT-2

### 2019 BERT å…ƒå¹´ ==> ä¸€å † XXXBERT
- Google BERT(Bidirectional Encoder Representation from Transformers) 
- ELMo
- ULMFiT
- GPT-1

### 2018
- FastText 
- GloVe
- word2Vec 

### 2017
- Google Transformer
  - [Transformeræ¨¡åž‹è¯¦è§£ï¼ˆå›¾è§£æœ€å®Œæ•´ç‰ˆï¼‰](https://zhuanlan.zhihu.com/p/338817680)

### Open-Source LLMs
- [The History of Open-Source LLMs: Better Base Models (Part Two)](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better)
