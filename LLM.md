# LLM(Large Language Models)
- [LLMæ˜¯ä»€éº¼ï¼Ÿè·ŸAIçš„é—œè¯ç‚ºä½•ï¼Ÿå¤§å‹èªè¨€æ¨¡å‹è¦é¢å°ä»€éº¼æŒ‘æˆ°ï¼Ÿä¸€æ–‡çœ‹æ‡‚](https://www.bnext.com.tw/article/76864/what-is-the-meaning-of-llm)

## æ•…äº‹çˆ†ç™¼åœ¨2017 
![Transformer(2017)](./pics/Transformer(2017).png)


## LLMç™¼å±•å² Evolutionary Tree

![LLMç™¼å±•å²](LLM_Tree.png)
- o	Open-Source
- o	Closed-Source
- o	Encoder-Only
- o	Encoder-Decoder
- o	Decoder- Only
### 2024
- Chat GLM(6B) [æ™ºè­œæ¸…è¨€-ChatGLM](https://chatglm.cn/)
  - [å®˜æ–¹èªªæ˜](https://github.com/THUDM/ChatGLM-6B)
  - ChatGLM-6B æ˜¯ä¸€å€‹é–‹æºçš„ã€æ”¯æŒä¸­è‹±é›™èªçš„å°è©±èªè¨€æ¨¡å‹ï¼ŒåŸºæ–¼ General Language Model (GLM) æ¶æ§‹ï¼Œå…·æœ‰ 62 å„„åƒæ•¸ã€‚
  - ChatGLM-6Bçµåˆæ¨¡å‹é‡åŒ–æŠ€è¡“ï¼Œä½¿ç”¨è€…å¯ä»¥åœ¨æ¶ˆè²»ç´šçš„é¡¯å¡ä¸Šé€²è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–ç´šåˆ¥ä¸‹æœ€ä½åªéœ€ 6GB é¡¯å­˜ï¼‰ã€‚
  - ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€è¡“ï¼Œé‡å°ä¸­æ–‡å•ç­”å’Œå°è©±é€²è¡Œäº†å„ªåŒ–ã€‚
  - ç¶“éç´„ 1T è­˜åˆ¥å­—çš„ä¸­è‹±é›™èªè¨“ç·´ï¼Œè¼”ä»¥ç›£ç£å¾®èª¿ã€å›é¥‹è‡ªåŠ©ã€äººé¡å›é¥‹å¼·åŒ–å­¸ç¿’ç­‰æŠ€è¡“çš„åŠ æŒï¼Œ62 å„„åƒæ•¸çš„ ChatGLM-6B å·²ç¶“èƒ½ç”Ÿæˆç›¸ç•¶ç¬¦åˆäººé¡åå¥½çš„å›ç­”ã€‚
  - chatglm.cn é«”é©—æ›´å¤§è¦æ¨¡çš„ ChatGLM æ¨¡å‹ã€‚ 
- Google Flan-UL2(20B)
- Meta LLaMA(65B)
  - LLaMA(Large Language Model Meta AI)æ˜¯Meta AIå…¬å¸æ–¼2023å¹´2æœˆæ¨å‡ºçš„å¤§å‹èªè¨€æ¨¡å‹
  - å› å…¶é–‹æºçš„ç‰¹æ€§èˆ‡å…¶æ“æœ‰ChatGPTç›¸ä¼¼çš„æ•ˆèƒ½ï¼Œä½¿å¾—è©²æ¨¡å‹å—åˆ°è¨±å¤šäººå–œæ„›
  - è¨±å¤šäººæœƒåŠ ä»¥å¾®èª¿ä»¥é”æˆé å®šçš„ç›®çš„ï¼Œç”šè‡³æœ‰è¨±å¤šäººå°‡é€™ç¨®æ¨¡å‹è¦–ç‚ºã€Œæœ¬åœ°ç«¯ChatGPTã€æˆ–ã€Œé–‹æºChatGPTã€
  - [ChatGPTçš„æŒ‘æˆ°è€…LLaMA(ä¸Š) - ç›®å‰æœ€å¼·å¤§çš„é–‹æºèªè¨€æ¨¡å‹LLaMAç©¶ç«Ÿåšäº†ä»€éº¼](https://ithelp.ithome.com.tw/articles/10338745)
  - [ChatGPTçš„æŒ‘æˆ°è€…LLaMA(ä¸‹) - ç”¨RLHFèˆ‡QLoRAèª¿æ•´å¤§å‹èªè¨€æ¨¡å‹](https://ithelp.ithome.com.tw/articles/10339382)
  - [How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models](https://promptengineering.org/how-does-llama-2-compare-to-gpt-and-other-ai-language-models/)
- Google Bard|â€Gemini  [â€Gemini:ç²å–å‰µæ„éˆæ„Ÿï¼Œæå‡å·¥ä½œæ•ˆç‡](https://gemini.google.com/?hl=zh-cn)
  - Bard AI æ˜¯ç”± Google åŸºæ–¼ LaMDA(Language Model for Dialogue Applications)æ‰€é–‹ç™¼çš„ AI èŠå¤©æ©Ÿå™¨äºº
  - å®ƒå¯ä»¥åƒæœå°‹å¼•æ“ä¸€æ¨£å›ç­”ç°¡å–®çš„å•é¡Œï¼Œä¸¦å°è¤‡é›œçš„å•é¡Œæä¾›å…¨é¢ä¸”å„ªè³ªçš„ç­”æ¡ˆã€‚
  - 2023å¹´5æœˆï¼ŒGoogleå®£å¸ƒæ›´æ”¹Bardåº•å±¤æ¨¡å‹ï¼Œå¾LaMDAèª¿æ•´ç‚ºPaLM 2ï¼Œèƒ½åŠ›è®Šå¾—æ›´å¼·
  - Google Bardæ›´åç‚ºâ€Gemini
- ğŸ‘OpenAI GPT-4 
- Jurassic-2 
- Claude 3
  - ç”±OpenAIå‰æˆå“¡å»ºç«‹çš„ Anthropicå…¬å¸é–‹ç™¼
  - ç‰¹è‰²:(æ¯”ChatGPTå²å®³å‘¢ï¼Ÿ)
    - å…è²»ä½¿ç”¨AIå°è©±æ©Ÿå™¨äºº
    - è³‡æ–™æ›´æ–°åˆ°2023å¹´åˆ
    - æ”¯æ´è¼¸å…¥100K token(è©èªæˆ–å­—ç¬¦çš„åŸºæœ¬å–®ä½)çš„è³‡æ–™å…§å®¹(GPT-4åªæœ‰32K token)
    - å¯ä»¥ä¸Šå‚³PDFæª”æ¡ˆå–å‡ºæ–‡å­—å’Œç”¢ç”Ÿæ‘˜è¦

### 2023
- ğŸ‘ChatGPT(20B)
-	ST-MoE
-	UL2
  - è«–æ–‡[UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)
-	Flan
-	T5
-	TK
- InstructGPT
- Sparrow
- OPT-IML
- BLOOMZ / BLOOM(176B)
- OPT
- Chinchilla
- Galactica 
- YaLM / PaLM(540B)
- Minerva
- LaMDA(173B)
- Flan PaLM
- Anthropic LM-v4-s3
- GPT-NeoX

### 2022
- GLM(General Language Model)
  - è«–æ–‡[GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)
  - [å®˜æ–¹GITHUBç¶²å€](https://github.com/THUDM/GLM)
- Switch 
- CodeX
- MT-NLG(530B)
- GLaM
- Jurassic-1
- Gopher
- ERNIE3.0
- LM
- Cohere
- GPT-J
- GPT-Neo

### 2021
- [Google ELECTRA](https://github.com/google-research/electra) 
- DeBERTa 
- mT5
- T0 
- GPT-3(175B)

### 2020
- RoBERTa
- ALBERT
- Distill BERT
- ERNIE 
- BART
- T5
- XL Net
- GPT-2

### 2019 BERT å…ƒå¹´ ==> ä¸€å † XXXBERT
- Google BERT(Bidirectional Encoder Representation from Transformers) 
- ELMo
- ULMFiT
- GPT-1


### 2017
- Google Transformer
  - [Transformeræ¨¡å‹è¯¦è§£ï¼ˆå›¾è§£æœ€å®Œæ•´ç‰ˆï¼‰](https://zhuanlan.zhihu.com/p/338817680)

### æ–‡å­—å‘é‡åŒ–
- FastText
  - https://fasttext.cc/
  - [fastTextåŸç†åŠå®è·µ - çŸ¥ä¹]() 
- å²ä¸¹ä½›å¤§å­¸ çš„GloVe(2014)
  - [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
  - [ç®€ä»‹GloVeè¯å‘é‡ï¼šæ¨å¯¼ã€å®ç°ã€åº”ç”¨ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/101179171)
- word2Vec(2014)
  - Google çš„ä¸€å€‹é–‹æºå·¥å…·ï¼Œèƒ½å¤ æ ¹æ“šè¼¸å…¥çš„ã€Œè©çš„é›†åˆã€è¨ˆç®—å‡ºè©èˆ‡è©ä¹‹é–“çš„è·é›¢

### Open-Source LLMs
- [The History of Open-Source LLMs: Better Base Models (Part Two)](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better)
